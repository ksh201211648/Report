{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -심화기초 기말고사 대체 과제-\n",
    "## 2021226599 인공지능학과 강성환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision\n",
    "!pip3 install torchsummary tensorboardX\n",
    "!pip3 install torch_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Origin Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.optim as optim #!#\n",
    "from torchsummary import summary #!#\n",
    "from tensorboardX import SummaryWriter #!#\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "root_dir = 'drive/app/cifar10/'\n",
    "default_directory = 'drive/app/torch/save_models/1_origin'\n",
    "\n",
    "writer = SummaryWriter('graph/1_origin') #!#\n",
    "\n",
    "# Data Augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "# automatically download\n",
    "train_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                 train=True,\n",
    "                                 transform=transform_train,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=4)           # CPU loader number\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=4)            # CPU loader number\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "modelses = model #!#\n",
    "modelses.cuda()#!#\n",
    "summary(modelses,(3,32,32)) #!#\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "            print(\"lr: \", optimizer.param_groups[0]['lr']) #!#\n",
    "\n",
    "            writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "\n",
    "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory)\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "for epoch in range(start_epoch, 165):\n",
    "\n",
    "    if epoch < 80:\n",
    "        lr = learning_rate\n",
    "    elif epoch < 120:\n",
    "        lr = learning_rate * 0.1\n",
    "    else:\n",
    "        lr = learning_rate * 0.01\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    train(epoch)\n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    })\n",
    "    test(epoch)  \n",
    "\n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os    \n",
    "import torchvision\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=0)           # CPU loader number\n",
    "\n",
    "# 이미지를 보여주기 위한 함수\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 학습용 이미지를 무작위로 가져오기\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"done_yet\")\n",
    "\n",
    "# 이미지 보여주기\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.optim as optim #!#\n",
    "from torchsummary import summary #!#\n",
    "from tensorboardX import SummaryWriter #!#\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 128\n",
    "learning_rate = 0.01 \n",
    "root_dir = 'drive/app/cifar10/'\n",
    "default_directory = 'drive/app/torch/save_models/2_resnet50'\n",
    "\n",
    "writer = SummaryWriter('graph/2_resnet50') #!#\n",
    "\n",
    "# Data Augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "# automatically download\n",
    "train_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                 train=True,\n",
    "                                 transform=transform_train,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=4)           # CPU loader number\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=4)            # CPU loader number\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module): #!#\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x  #!#\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#model = ResNet(BasicBlock, [2, 2, 2, 2]) \n",
    "model = ResNet(BottleNeck, [3,4,6,3])  #!#\n",
    "\n",
    "modelses = model #!#\n",
    "modelses.cuda()#!#\n",
    "summary(modelses,(3,32,32)) #!#\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "            print(\"lr: \", optimizer.param_groups[0]['lr']) #!#\n",
    "\n",
    "            writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory)\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "for epoch in range(start_epoch, 165):\n",
    "    if epoch < 15:\n",
    "        lr = learning_rate\n",
    "    elif epoch < 75:\n",
    "        lr = learning_rate * 0.5\n",
    "    else:\n",
    "        lr = learning_rate * 0.05\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    train(epoch)\n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    })\n",
    "    test(epoch)  #!#\n",
    "\n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os    \n",
    "import torchvision\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=0)           # CPU loader number\n",
    "\n",
    "# 이미지를 보여주기 위한 함수\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 학습용 이미지를 무작위로 가져오기\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"done_yet\")\n",
    "\n",
    "# 이미지 보여주기\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. resnet50_label_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.optim as optim #!#\n",
    "from torchsummary import summary #!#\n",
    "from tensorboardX import SummaryWriter #!#\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 128\n",
    "learning_rate = 0.001 \n",
    "root_dir = 'drive/app/cifar10/'\n",
    "default_directory = 'drive/app/torch/save_models/3_resnet50_label_smoothing'\n",
    "\n",
    "writer = SummaryWriter('graph/3_resnet50_label_smoothing') #!#\n",
    "\n",
    "# Data Augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "# automatically download\n",
    "train_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                 train=True,\n",
    "                                 transform=transform_train,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=4)           # CPU loader number\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=4)            # CPU loader number\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module): #!#\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x  #!#\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#model = ResNet(BasicBlock, [2, 2, 2, 2]) \n",
    "model = ResNet(BottleNeck, [3,4,6,3])  #!#\n",
    "\n",
    "modelses = model #!#\n",
    "modelses.cuda()#!#\n",
    "summary(modelses,(3,32,32)) #!#\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module): #!#\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim)) #!#\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)\n",
    "criterion = LabelSmoothingLoss(classes = 10, smoothing = 0.1) #!#\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "            print(\"lr: \", optimizer.param_groups[0]['lr']) #!#\n",
    "\n",
    "            writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory)\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "for epoch in range(start_epoch, 165):\n",
    "    if epoch < 110:\n",
    "        lr = learning_rate\n",
    "    elif epoch < 140:\n",
    "        lr = learning_rate * 0.1\n",
    "    else:\n",
    "        lr = learning_rate * 0.01\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    train(epoch)\n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    })\n",
    "    test(epoch)  #!#\n",
    "\n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os    \n",
    "import torchvision\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=0)           # CPU loader number\n",
    "\n",
    "# 이미지를 보여주기 위한 함수\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 학습용 이미지를 무작위로 가져오기\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"done_yet\")\n",
    "\n",
    "# 이미지 보여주기\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. resnet50_cutmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "#import torch.optim as optim #!#\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "\n",
    "import torch_optimizer as optim #!#\n",
    "from torchsummary import summary #!#\n",
    "from tensorboardX import SummaryWriter #!#\n",
    "\n",
    "import random\n",
    "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
    "from PIL import Image\n",
    "import torch.optim as optimals\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "batch_size = 128\n",
    "root_dir = 'drive/app/cifar10/'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "learning_rate = 0.01 \n",
    "default_directory5 = 'drive/app/torch/save_models/4_resnet50_cutmix'\n",
    "\n",
    "writer = SummaryWriter('graph/4_resnet50_cutmix') #!#\n",
    "\n",
    "# Data Augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "# automatically download\n",
    "train_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                 train=True,\n",
    "                                 transform=transform_train,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=4)           # CPU loader number\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=4)            # CPU loader number\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module): #!#\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x  #!#\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#model = ResNet(BasicBlock, [2, 2, 2, 2]) \n",
    "model5 = ResNet(BottleNeck, [3,4,6,3])  #!#\n",
    "\n",
    "modelses = model5 #!#\n",
    "modelses.cuda()#!#\n",
    "summary(modelses,(3,32,32)) #!#\n",
    "\n",
    "optimizer5 = optimals.SGD(model5.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)\n",
    "criterion5 = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model5 = nn.DataParallel(model5).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "def rand_bbox(size, lam): #!#\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2 #!#\n",
    "\n",
    "def train(epoch):\n",
    "    model5.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        r = np.random.rand(1) #!#\n",
    "        if r < 0.5:\n",
    "            # generate mixed sample\n",
    "            lam = np.random.beta(1.0, 1.0)\n",
    "            rand_index = torch.randperm(data.size()[0]).cuda()\n",
    "            target_a = target\n",
    "            target_b = target[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "            data[:, :, bbx1:bbx2, bby1:bby2] = data[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            # adjust lambda to exactly match pixel ratio\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "            # compute output\n",
    "            output = model5(data)\n",
    "            loss = criterion5(output, target_a) * lam + criterion5(output, target_b) * (1. - lam)\n",
    "        else:\n",
    "            # compute output\n",
    "            output = model5(data)\n",
    "            loss = criterion5(output, target) #!#\n",
    "        \n",
    "        optimizer5.zero_grad()\n",
    "        #output = model(data) #!#\n",
    "        #loss = criterion(output, target) #!#\n",
    "        loss.backward()\n",
    "        optimizer5.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "            print(\"lr: \", optimizer5.param_groups[0]['lr']) #!#\n",
    "\n",
    "            writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('lr', optimizer5.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#\n",
    "\n",
    "def test(epoch):\n",
    "    model5.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model5(data)\n",
    "        loss = criterion5(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint5 = load_checkpoint(default_directory5)\n",
    "if not checkpoint5:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint5['epoch'] + 1\n",
    "    model5.load_state_dict(checkpoint5['state_dict']) #!# model1.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer5.load_state_dict(checkpoint5['optimizer'])\n",
    "\n",
    "for epoch in range(start_epoch, 165):\n",
    "    if epoch < 50:\n",
    "        lr = learning_rate\n",
    "    elif epoch < 100:\n",
    "        lr = learning_rate * 0.5\n",
    "    else:\n",
    "        lr = learning_rate * 0.01\n",
    "    for param_group in optimizer5.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    train(epoch)\n",
    "    save_checkpoint(default_directory5, {\n",
    "        'epoch': epoch,\n",
    "        'model': model5,\n",
    "        'state_dict': model5.state_dict(), # 'state_dict': model5.state_dict() #!#\n",
    "        'optimizer': optimizer5.state_dict(), # 'optimizer': optimizer5.state_dict() #!#\n",
    "    })\n",
    "    test(epoch)  #!#\n",
    "\n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os    \n",
    "import torchvision\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=0)           # CPU loader number\n",
    "\n",
    "#!#\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    # if torch.cuda.is_available():\n",
    "    #     data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "    # else:\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    r = np.random.rand(1) #!#\n",
    "    if r < 0.5:\n",
    "        # generate mixed sample\n",
    "        lam = np.random.beta(1.0, 1.0)\n",
    "        rand_index = torch.randperm(data.size()[0]).cuda()\n",
    "        target_a = target\n",
    "        target_b = target[rand_index]\n",
    "        bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "        data[:, :, bbx1:bbx2, bby1:bby2] = data[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "#!#\n",
    "\n",
    "# image_batch, label_batch = next(iter(train_loader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(4):\n",
    "    ax = plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(np.transpose(data[i], (1, 2, 0)))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.resnet50_lamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "#import torch.optim as optim #!#\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "\n",
    "import torch_optimizer as optim #!#\n",
    "from torchsummary import summary #!#\n",
    "from tensorboardX import SummaryWriter #!#\n",
    "\n",
    "import random\n",
    "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
    "from PIL import Image\n",
    "import torch.optim as optimals\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "batch_size = 128\n",
    "root_dir = 'drive/app/cifar10/'\n",
    "\n",
    "learning_rate = 0.001 \n",
    "default_directory1 = 'drive/app/torch/save_models/5_resnet50_lamb'\n",
    "start_time = time.time()\n",
    "writer = SummaryWriter('graph/5_resnet50_lamb') #!#\n",
    "\n",
    "# Data Augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "# automatically download\n",
    "train_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                 train=True,\n",
    "                                 transform=transform_train,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=4)           # CPU loader number\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=4)            # CPU loader number\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module): #!#\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x  #!#\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#model = ResNet(BasicBlock, [2, 2, 2, 2]) \n",
    "model1 = ResNet(BottleNeck, [3,4,6,3])  #!#\n",
    "\n",
    "modelses = model1 #!#\n",
    "modelses.cuda()#!#\n",
    "summary(modelses,(3,32,32)) #!#\n",
    "\n",
    "optimizer1 = optim.Lamb( #!#\n",
    "    model1.parameters(),\n",
    "    lr= 1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0,\n",
    ") #!#\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model1 = nn.DataParallel(model1).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model1.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        output = model1(data)\n",
    "        loss = criterion1(output, target)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "            print(\"lr: \", optimizer1.param_groups[0]['lr']) #!#\n",
    "\n",
    "            writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('lr', optimizer1.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#\n",
    "\n",
    "def test(epoch):\n",
    "    model1.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model1(data)\n",
    "        loss = criterion1(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint1 = load_checkpoint(default_directory1)\n",
    "if not checkpoint1:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint1['epoch'] + 1\n",
    "    model1.load_state_dict(checkpoint1['state_dict']) #!# model1.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer1.load_state_dict(checkpoint1['optimizer'])\n",
    "\n",
    "for epoch in range(start_epoch, 165):\n",
    "    if epoch < 80:\n",
    "        lr = learning_rate\n",
    "    elif epoch < 120:\n",
    "        lr = learning_rate * 0.1\n",
    "    else:\n",
    "        lr = learning_rate * 0.01\n",
    "    for param_group in optimizer1.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    train(epoch)\n",
    "    save_checkpoint(default_directory1, {\n",
    "        'epoch': epoch,\n",
    "        'model': model1,\n",
    "        'state_dict': model1.state_dict(), # 'state_dict': model1.state_dict() #!#\n",
    "        'optimizer': optimizer1.state_dict(), # 'optimizer': optimizer1.state_dict() #!#\n",
    "    })\n",
    "    test(epoch)  #!#\n",
    "\n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os    \n",
    "import torchvision\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=0)           # CPU loader number\n",
    "\n",
    "# 이미지를 보여주기 위한 함수\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 학습용 이미지를 무작위로 가져오기\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"done_yet\")\n",
    "\n",
    "# 이미지 보여주기\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.resnet50_mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "#import torch.optim as optim #!#\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "\n",
    "import torch_optimizer as optim #!#\n",
    "from torchsummary import summary #!#\n",
    "from tensorboardX import SummaryWriter #!#\n",
    "\n",
    "import random\n",
    "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
    "from PIL import Image\n",
    "import torch.optim as optimals\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "batch_size = 128\n",
    "root_dir = 'drive/app/cifar10/'\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "default_directory2 = 'drive/app/torch/save_models/6_resnet50_mixup'\n",
    "writer = SummaryWriter('graph/6_resnet50_mixup') #!#\n",
    "start_time = time.time()\n",
    "\n",
    "# Data Augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "# automatically download\n",
    "train_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                 train=True,\n",
    "                                 transform=transform_train,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=4)           # CPU loader number\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=4)            # CPU loader number\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module): #!#\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x  #!#\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#model = ResNet(BasicBlock, [2, 2, 2, 2]) \n",
    "model2 = ResNet(BottleNeck, [3,4,6,3])  #!#\n",
    "\n",
    "modelses = model2 #!#\n",
    "modelses.cuda()#!#\n",
    "summary(modelses,(3,32,32)) #!#\n",
    "\n",
    "optimizer2 = optimals.SGD(model2.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0): #!#\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if torch.cuda.is_available():\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam \n",
    "    \n",
    "def mixup_criterion(criterion2, pred, y_a, y_b, lam):\n",
    "    return lam * criterion2(pred, y_a) + (1 - lam) * criterion2(pred, y_b) #!#\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model2 = nn.DataParallel(model2).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model2.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer2.zero_grad()\n",
    "        #loss = criterion(output, target)\n",
    "        data, targets_a, targets_b, lam = mixup_data(data, target, alpha = 0.1) #!#\n",
    "        output = model2(data)\n",
    "        loss = mixup_criterion(criterion2, output, targets_a, targets_b, lam) #!#\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "            print(\"lr: \", optimizer2.param_groups[0]['lr']) #!#\n",
    "\n",
    "            writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('lr', optimizer2.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#\n",
    "\n",
    "def test(epoch):\n",
    "    model2.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model2(data)\n",
    "        loss = criterion2(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint2 = load_checkpoint(default_directory2)\n",
    "if not checkpoint2:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint2['epoch'] + 1\n",
    "    model2.load_state_dict(checkpoint2['state_dict']) #!# model1.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer2.load_state_dict(checkpoint2['optimizer'])\n",
    "\n",
    "for epoch in range(start_epoch, 165):\n",
    "    if epoch < 100:\n",
    "        lr = learning_rate\n",
    "    elif epoch < 130:\n",
    "        lr = learning_rate * 0.1\n",
    "    else:\n",
    "        lr = learning_rate * 0.01\n",
    "    for param_group in optimizer2.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    train(epoch)\n",
    "    save_checkpoint(default_directory2, {\n",
    "        'epoch': epoch,\n",
    "        'model': model2,\n",
    "        'state_dict': model2.state_dict(), # 'state_dict': model2.state_dict() #!#\n",
    "        'optimizer': optimizer2.state_dict(), # 'optimizer': optimizer2.state_dict() #!#\n",
    "    })\n",
    "    test(epoch)  #!#\n",
    "\n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os    \n",
    "import torchvision\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=8,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=0)           # CPU loader number\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    # if torch.cuda.is_available():\n",
    "    #     data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "    # else:\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    data, targets_a, targets_b, lam = mixup_data(data, target, alpha = 0.5) #!#\n",
    "\n",
    "# 이미지를 보여주기 위한 함수\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# image_batch, label_batch = next(iter(train_loader))\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(8):\n",
    "    ax = plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(np.transpose(data[i], (1, 2, 0)))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.resnet50_randaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "#import torch.optim as optim #!#\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "\n",
    "import torch_optimizer as optim #!#\n",
    "from torchsummary import summary #!#\n",
    "from tensorboardX import SummaryWriter #!#\n",
    "\n",
    "import random\n",
    "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
    "from PIL import Image\n",
    "import torch.optim as optimals\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "batch_size = 128\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def ShearX(img, v):  # [-0.3, 0.3]\n",
    "    assert -0.3 <= v <= 0.3\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n",
    "\n",
    "\n",
    "def ShearY(img, v):  # [-0.3, 0.3]\n",
    "    assert -0.3 <= v <= 0.3\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n",
    "\n",
    "\n",
    "def TranslateX(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n",
    "    assert -0.45 <= v <= 0.45\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    v = v * img.size[0]\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n",
    "\n",
    "\n",
    "def TranslateXabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n",
    "    assert 0 <= v\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n",
    "\n",
    "\n",
    "def TranslateY(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n",
    "    assert -0.45 <= v <= 0.45\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    v = v * img.size[1]\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n",
    "\n",
    "\n",
    "def TranslateYabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n",
    "    assert 0 <= v\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n",
    "\n",
    "\n",
    "def Rotate(img, v):  # [-30, 30]\n",
    "    assert -30 <= v <= 30\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    return img.rotate(v)\n",
    "\n",
    "\n",
    "def AutoContrast(img, _):\n",
    "    return PIL.ImageOps.autocontrast(img)\n",
    "\n",
    "\n",
    "def Invert(img, _):\n",
    "    return PIL.ImageOps.invert(img)\n",
    "\n",
    "\n",
    "def Equalize(img, _):\n",
    "    return PIL.ImageOps.equalize(img)\n",
    "\n",
    "\n",
    "def Flip(img, _):  # not from the paper\n",
    "    return PIL.ImageOps.mirror(img)\n",
    "\n",
    "\n",
    "def Solarize(img, v):  # [0, 256]\n",
    "    assert 0 <= v <= 256\n",
    "    return PIL.ImageOps.solarize(img, v)\n",
    "\n",
    "\n",
    "def SolarizeAdd(img, addition=0, threshold=128):\n",
    "    img_np = np.array(img).astype(np.int)\n",
    "    img_np = img_np + addition\n",
    "    img_np = np.clip(img_np, 0, 255)\n",
    "    img_np = img_np.astype(np.uint8)\n",
    "    img = Image.fromarray(img_np)\n",
    "    return PIL.ImageOps.solarize(img, threshold)\n",
    "\n",
    "\n",
    "def Posterize(img, v):  # [4, 8]\n",
    "    v = int(v)\n",
    "    v = max(1, v)\n",
    "    return PIL.ImageOps.posterize(img, v)\n",
    "\n",
    "\n",
    "def Contrast(img, v):  # [0.1,1.9]\n",
    "    assert 0.1 <= v <= 1.9\n",
    "    return PIL.ImageEnhance.Contrast(img).enhance(v)\n",
    "\n",
    "\n",
    "def Color(img, v):  # [0.1,1.9]\n",
    "    assert 0.1 <= v <= 1.9\n",
    "    return PIL.ImageEnhance.Color(img).enhance(v)\n",
    "\n",
    "\n",
    "def Brightness(img, v):  # [0.1,1.9]\n",
    "    assert 0.1 <= v <= 1.9\n",
    "    return PIL.ImageEnhance.Brightness(img).enhance(v)\n",
    "\n",
    "\n",
    "def Sharpness(img, v):  # [0.1,1.9]\n",
    "    assert 0.1 <= v <= 1.9\n",
    "    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n",
    "\n",
    "\n",
    "def Cutout(img, v):  # [0, 60] => percentage: [0, 0.2]\n",
    "    assert 0.0 <= v <= 0.2\n",
    "    if v <= 0.:\n",
    "        return img\n",
    "\n",
    "    v = v * img.size[0]\n",
    "    return CutoutAbs(img, v)\n",
    "\n",
    "\n",
    "def CutoutAbs(img, v):  # [0, 60] => percentage: [0, 0.2]\n",
    "    # assert 0 <= v <= 20\n",
    "    if v < 0:\n",
    "        return img\n",
    "    w, h = img.size\n",
    "    x0 = np.random.uniform(w)\n",
    "    y0 = np.random.uniform(h)\n",
    "\n",
    "    x0 = int(max(0, x0 - v / 2.))\n",
    "    y0 = int(max(0, y0 - v / 2.))\n",
    "    x1 = min(w, x0 + v)\n",
    "    y1 = min(h, y0 + v)\n",
    "\n",
    "    xy = (x0, y0, x1, y1)\n",
    "    color = (125, 123, 114)\n",
    "    # color = (0, 0, 0)\n",
    "    img = img.copy()\n",
    "    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n",
    "    return img\n",
    "\n",
    "\n",
    "def SamplePairing(imgs):  # [0, 0.4]\n",
    "    def f(img1, v):\n",
    "        i = np.random.choice(len(imgs))\n",
    "        img2 = PIL.Image.fromarray(imgs[i])\n",
    "        return PIL.Image.blend(img1, img2, v)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def Identity(img, v):\n",
    "    return img\n",
    "\n",
    "\n",
    "def augment_list():  # 16 oeprations and their ranges\n",
    "    # https://github.com/google-research/uda/blob/master/image/randaugment/policies.py#L57\n",
    "    # l = [\n",
    "    #     (Identity, 0., 1.0),\n",
    "    #     (ShearX, 0., 0.3),  # 0\n",
    "    #     (ShearY, 0., 0.3),  # 1\n",
    "    #     (TranslateX, 0., 0.33),  # 2\n",
    "    #     (TranslateY, 0., 0.33),  # 3\n",
    "    #     (Rotate, 0, 30),  # 4\n",
    "    #     (AutoContrast, 0, 1),  # 5\n",
    "    #     (Invert, 0, 1),  # 6\n",
    "    #     (Equalize, 0, 1),  # 7\n",
    "    #     (Solarize, 0, 110),  # 8\n",
    "    #     (Posterize, 4, 8),  # 9\n",
    "    #     # (Contrast, 0.1, 1.9),  # 10\n",
    "    #     (Color, 0.1, 1.9),  # 11\n",
    "    #     (Brightness, 0.1, 1.9),  # 12\n",
    "    #     (Sharpness, 0.1, 1.9),  # 13\n",
    "    #     # (Cutout, 0, 0.2),  # 14\n",
    "    #     # (SamplePairing(imgs), 0, 0.4),  # 15\n",
    "    # ]\n",
    "\n",
    "    # https://github.com/tensorflow/tpu/blob/8462d083dd89489a79e3200bcc8d4063bf362186/models/official/efficientnet/autoaugment.py#L505\n",
    "    l = [\n",
    "        (AutoContrast, 0, 1),\n",
    "        (Equalize, 0, 1),\n",
    "        (Invert, 0, 1),\n",
    "        (Rotate, 0, 30),\n",
    "        (Posterize, 0, 4),\n",
    "        (Solarize, 0, 256),\n",
    "        (SolarizeAdd, 0, 110),\n",
    "        (Color, 0.1, 1.9),\n",
    "        (Contrast, 0.1, 1.9),\n",
    "        (Brightness, 0.1, 1.9),\n",
    "        (Sharpness, 0.1, 1.9),\n",
    "        (ShearX, 0., 0.3),\n",
    "        (ShearY, 0., 0.3),\n",
    "        (CutoutAbs, 0, 40),\n",
    "        (TranslateXabs, 0., 100),\n",
    "        (TranslateYabs, 0., 100),\n",
    "    ]\n",
    "\n",
    "    return l\n",
    "\n",
    "\n",
    "class Lighting(object):\n",
    "    \"\"\"Lighting noise(AlexNet - style PCA - based noise)\"\"\"\n",
    "\n",
    "    def __init__(self, alphastd, eigval, eigvec):\n",
    "        self.alphastd = alphastd\n",
    "        self.eigval = torch.Tensor(eigval)\n",
    "        self.eigvec = torch.Tensor(eigvec)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.alphastd == 0:\n",
    "            return img\n",
    "\n",
    "        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n",
    "        rgb = self.eigvec.type_as(img).clone() \\\n",
    "            .mul(alpha.view(1, 3).expand(3, 3)) \\\n",
    "            .mul(self.eigval.view(1, 3).expand(3, 3)) \\\n",
    "            .sum(1).squeeze()\n",
    "\n",
    "        return img.add(rgb.view(3, 1, 1).expand_as(img))\n",
    "\n",
    "\n",
    "class CutoutDefault(object):\n",
    "    \"\"\"\n",
    "    Reference : https://github.com/quark0/darts/blob/master/cnn/utils.py\n",
    "    \"\"\"\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h, w = img.size(1), img.size(2)\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "\n",
    "        y1 = np.clip(y - self.length // 2, 0, h)\n",
    "        y2 = np.clip(y + self.length // 2, 0, h)\n",
    "        x1 = np.clip(x - self.length // 2, 0, w)\n",
    "        x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "        mask[y1: y2, x1: x2] = 0.\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img *= mask\n",
    "        return img\n",
    "\n",
    "\n",
    "class RandAugment:\n",
    "    def __init__(self, n, m):\n",
    "        self.n = n\n",
    "        self.m = m      # [0, 30]\n",
    "        self.augment_list = augment_list()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        ops = random.choices(self.augment_list, k=self.n)\n",
    "        for op, minval, maxval in ops:\n",
    "            val = (float(self.m) / 30) * float(maxval - minval) + minval\n",
    "            img = op(img, val)\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "learning_rate = 0.01 \n",
    "root_dir = 'drive/app/cifar10/'\n",
    "default_directory3 = 'drive/app/torch/save_models/7_resnet50_randaug'\n",
    "writer = SummaryWriter('graph/7_resnet50_randaug') #!#\n",
    "\n",
    "# Data Augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_train.transforms.insert(0, RandAugment(2, 11)) \n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "# automatically download\n",
    "train_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                 train=True,\n",
    "                                 transform=transform_train,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=0)           # CPU loader number\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=0)            # CPU loader number\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module): #!#\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x  #!#\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#model = ResNet(BasicBlock, [2, 2, 2, 2]) \n",
    "model3 = ResNet(BottleNeck, [3,4,6,3])  #!#\n",
    "\n",
    "modelses = model3 #!#\n",
    "modelses.cuda()#!#\n",
    "summary(modelses,(3,32,32)) #!#\n",
    "\n",
    "optimizer3 = optimals.SGD(model3.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)\n",
    "criterion3 = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model3 = nn.DataParallel(model3).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model3.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer3.zero_grad()\n",
    "        output = model3(data)\n",
    "        loss = criterion3(output, target)\n",
    "        loss.backward()\n",
    "        optimizer3.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "            print(\"lr: \", optimizer3.param_groups[0]['lr']) #!#\n",
    "\n",
    "            writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('lr', optimizer3.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#\n",
    "\n",
    "def test(epoch):\n",
    "    model3.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model3(data)\n",
    "        loss = criterion3(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint3 = load_checkpoint(default_directory3)\n",
    "if not checkpoint3:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint3['epoch'] + 1\n",
    "    model3.load_state_dict(checkpoint3['state_dict']) #!# model1.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer3.load_state_dict(checkpoint3['optimizer'])\n",
    "\n",
    "for epoch in range(start_epoch, 165): \n",
    "    if epoch < 15:\n",
    "        lr = learning_rate\n",
    "    elif epoch < 75:\n",
    "        lr = learning_rate * 0.5\n",
    "    else:\n",
    "        lr = learning_rate * 0.05\n",
    "    for param_group in optimizer3.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    train(epoch)\n",
    "    save_checkpoint(default_directory3, {\n",
    "        'epoch': epoch,\n",
    "        'model': model3,\n",
    "        'state_dict': model3.state_dict(), # 'state_dict': model3.state_dict() #!#\n",
    "        'optimizer': optimizer3.state_dict(), # 'optimizer': optimizer3.state_dict() #!#\n",
    "    })\n",
    "    test(epoch)  #!#\n",
    "\n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os    \n",
    "import torchvision\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=0)           # CPU loader number\n",
    "\n",
    "# 이미지를 보여주기 위한 함수\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 학습용 이미지를 무작위로 가져오기\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"done_yet\")\n",
    "\n",
    "# 이미지 보여주기\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.resnet50_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "#import torch.optim as optim #!#\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "\n",
    "import torch_optimizer as optim #!#\n",
    "from torchsummary import summary #!#\n",
    "from tensorboardX import SummaryWriter #!#\n",
    "\n",
    "import random\n",
    "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
    "from PIL import Image\n",
    "import torch.optim as optimals\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "batch_size = 128\n",
    "root_dir = 'drive/app/cifar10/'\n",
    "\n",
    "learning_rate = 0.01 \n",
    "start_time = time.time()\n",
    "default_directory4 = 'drive/app/torch/save_models/8_resnet50_scheduler'\n",
    "\n",
    "writer = SummaryWriter('graph/8_resnet50_scheduler') #!#\n",
    "\n",
    "# Data Augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "# automatically download\n",
    "train_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                 train=True,\n",
    "                                 transform=transform_train,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=4)           # CPU loader number\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=4)            # CPU loader number\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class BottleNeck(nn.Module): #!#\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x  #!#\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model4 = ResNet(BottleNeck, [3,4,6,3]) #!#\n",
    "\n",
    "modelses = model4 #!#\n",
    "modelses.cuda()#!#\n",
    "summary(modelses,(3,32,32)) #!#\n",
    "\n",
    "optimizer4 = optimals.SGD(model4.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)\n",
    "criterion4 = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model4 = nn.DataParallel(model4).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model4.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer4.zero_grad()\n",
    "        output = model4(data)\n",
    "        loss = criterion4(output, target)\n",
    "        loss.backward()\n",
    "        optimizer4.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "            print(\"lr: \", optimizer4.param_groups[0]['lr']) #!#\n",
    "\n",
    "            writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('lr', optimizer4.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#\n",
    "\n",
    "def test(epoch):\n",
    "    model4.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model4(data)\n",
    "        loss = criterion4(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint4 = load_checkpoint(default_directory4)\n",
    "if not checkpoint4:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint4['epoch'] + 1\n",
    "    model4.load_state_dict(checkpoint4['state_dict']) #!# model1.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer4.load_state_dict(checkpoint4['optimizer'])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer4, T_max=100, eta_min=0, last_epoch=- 1)\n",
    "\n",
    "for epoch in range(start_epoch, 100):\n",
    "\n",
    "    train(epoch)\n",
    "    if epoch < 25: \n",
    "        lr = learning_rate\n",
    "        for param_group in optimizer4.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    else:\n",
    "        scheduler.step() #!#\n",
    "    save_checkpoint(default_directory4, {\n",
    "        'epoch': epoch,\n",
    "        'model': model4,\n",
    "        'state_dict': model4.state_dict(), # 'state_dict': model4.state_dict() #!#\n",
    "        'optimizer': optimizer4.state_dict(), # 'optimizer': optimizer4.state_dict() #!#\n",
    "    })\n",
    "    test(epoch)  #!#\n",
    "\n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os    \n",
    "import torchvision\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=0)           # CPU loader number\n",
    "\n",
    "# 이미지를 보여주기 위한 함수\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 학습용 이미지를 무작위로 가져오기\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"done_yet\")\n",
    "\n",
    "# 이미지 보여주기\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.ensemble(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_directory1 =  'drive/app/torch/save_models/5_resnet50_lamb'\n",
    "default_directory2 =  'drive/app/torch/save_models/6_resnet50_mixup'\n",
    "default_directory3 =  'drive/app/torch/save_models/7_resnet50_randaug'\n",
    "default_directory4 =  'drive/app/torch/save_models/8_resnet50_scheduler'\n",
    "default_directory5 =  'drive/app/torch/save_models/4_resnet50_cutmix'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=16,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=0)            # CPU loader number\n",
    "\n",
    "class BottleNeck(nn.Module): #!#\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x  #!#\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model1 = ResNet(BottleNeck, [3,4,6,3])\n",
    "model2 = ResNet(BottleNeck, [3,4,6,3])\n",
    "model3 = ResNet(BottleNeck, [3,4,6,3])\n",
    "model4 = ResNet(BottleNeck, [3,4,6,3])\n",
    "model5 = ResNet(BottleNeck, [3,4,6,3])\n",
    "\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "criterion3 = nn.CrossEntropyLoss()\n",
    "criterion4 = nn.CrossEntropyLoss()\n",
    "criterion5 = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model1 = nn.DataParallel(model1).cuda()\n",
    "    model2 = nn.DataParallel(model2).cuda()\n",
    "    model3 = nn.DataParallel(model3).cuda()\n",
    "    model4 = nn.DataParallel(model4).cuda()\n",
    "    model5 = nn.DataParallel(model5).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    model3.eval()\n",
    "    model4.eval()\n",
    "    model5.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs1 = model1(data)\n",
    "        loss1 = criterion1(outputs1, target)\n",
    "        \n",
    "        outputs2 = model2(data)\n",
    "        loss2 = criterion2(outputs2, target)\n",
    "        \n",
    "        outputs3 = model3(data)\n",
    "        loss3 = criterion3(outputs3, target)\n",
    "        \n",
    "        outputs4 = model4(data)\n",
    "        loss4 = criterion4(outputs4, target)\n",
    "        \n",
    "        outputs5 = model5(data)\n",
    "        loss5 = criterion5(outputs5, target)\n",
    "        \n",
    "        loss = ( loss1+loss2+loss3+loss4+loss5 )/ 5\n",
    "        outputs = (outputs1 + outputs2 + outputs3 + outputs4+outputs5)/5\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        \n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint1 = load_checkpoint(default_directory1)\n",
    "checkpoint2 = load_checkpoint(default_directory2)\n",
    "checkpoint3 = load_checkpoint(default_directory3)\n",
    "checkpoint4 = load_checkpoint(default_directory4)\n",
    "checkpoint5 = load_checkpoint(default_directory5)\n",
    "\n",
    "model1.load_state_dict(checkpoint1['state_dict']) #model1.load_state_dict(checkpoint2['state_dict']) #!#\n",
    "model2.load_state_dict(checkpoint2['state_dict'])\n",
    "model3.load_state_dict(checkpoint3['state_dict'])\n",
    "model4.load_state_dict(checkpoint4['state_dict'])\n",
    "model5.load_state_dict(checkpoint5['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 1):\n",
    "    test(epoch)  \n",
    "\n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.ensemble(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_directory1 =  'drive/app/torch/save_models/5_resnet50_lamb'\n",
    "default_directory2 =  'drive/app/torch/save_models/6_resnet50_mixup'\n",
    "#default_directory3 =  'drive/app/torch/save_models/7_resnet50_randaug'\n",
    "#default_directory4 =  'drive/app/torch/save_models/8_resnet50_scheduler'\n",
    "default_directory5 =  'drive/app/torch/save_models/4_resnet50_cutmix'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=16,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=0)            # CPU loader number\n",
    "\n",
    "class BottleNeck(nn.Module): #!#\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x  #!#\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model1 = ResNet(BottleNeck, [3,4,6,3])\n",
    "model2 = ResNet(BottleNeck, [3,4,6,3])\n",
    "#model3 = ResNet(BottleNeck, [3,4,6,3])\n",
    "#model4 = ResNet(BottleNeck, [3,4,6,3])\n",
    "model5 = ResNet(BottleNeck, [3,4,6,3])\n",
    "\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "#criterion3 = nn.CrossEntropyLoss()\n",
    "#criterion4 = nn.CrossEntropyLoss()\n",
    "criterion5 = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model1 = nn.DataParallel(model1).cuda()\n",
    "    model2 = nn.DataParallel(model2).cuda()\n",
    "    #model3 = nn.DataParallel(model3).cuda()\n",
    "    #model4 = nn.DataParallel(model4).cuda()\n",
    "    model5 = nn.DataParallel(model5).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    #model3.eval()\n",
    "    #model4.eval()\n",
    "    model5.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs1 = model1(data)\n",
    "        loss1 = criterion1(outputs1, target)\n",
    "        \n",
    "        outputs2 = model2(data)\n",
    "        loss2 = criterion2(outputs2, target)\n",
    "        \n",
    "        #outputs3 = model3(data)\n",
    "        #loss3 = criterion3(outputs3, target)\n",
    "        \n",
    "        #outputs4 = model4(data)\n",
    "        #loss4 = criterion4(outputs4, target)\n",
    "        \n",
    "        outputs5 = model5(data)\n",
    "        loss5 = criterion5(outputs5, target)\n",
    "        \n",
    "        loss = ( loss1+loss2+loss5 )/ 3\n",
    "        outputs = (outputs1 + outputs2 +outputs5)/3\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        \n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint1 = load_checkpoint(default_directory1)\n",
    "checkpoint2 = load_checkpoint(default_directory2)\n",
    "#checkpoint3 = load_checkpoint(default_directory3)\n",
    "#checkpoint4 = load_checkpoint(default_directory4)\n",
    "checkpoint5 = load_checkpoint(default_directory5)\n",
    "\n",
    "model1.load_state_dict(checkpoint1['state_dict']) #model1.load_state_dict(checkpoint2['state_dict']) #!#\n",
    "model2.load_state_dict(checkpoint2['state_dict'])\n",
    "#model3.load_state_dict(checkpoint3['state_dict'])\n",
    "#model4.load_state_dict(checkpoint4['state_dict'])\n",
    "model5.load_state_dict(checkpoint5['state_dict'])\n",
    "\n",
    "for epoch in range(start_epoch, 1):\n",
    "    test(epoch)  \n",
    "\n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.use_all_thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision #!#\n",
    "import matplotlib.pyplot as plt #!#\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch_optimizer as optim #!#\n",
    "from torchsummary import summary #!#\n",
    "from tensorboardX import SummaryWriter #!#\n",
    "\n",
    "import random #!#\n",
    "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw #!#\n",
    "import numpy as np #!#\n",
    "from PIL import Image #!#\n",
    "\n",
    "def ShearX(img, v):  # [-0.3, 0.3] #!#\n",
    "    assert -0.3 <= v <= 0.3\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n",
    "\n",
    "\n",
    "def ShearY(img, v):  # [-0.3, 0.3]\n",
    "    assert -0.3 <= v <= 0.3\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n",
    "\n",
    "\n",
    "def TranslateX(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n",
    "    assert -0.45 <= v <= 0.45\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    v = v * img.size[0]\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n",
    "\n",
    "\n",
    "def TranslateXabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n",
    "    assert 0 <= v\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n",
    "\n",
    "\n",
    "def TranslateY(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n",
    "    assert -0.45 <= v <= 0.45\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    v = v * img.size[1]\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n",
    "\n",
    "\n",
    "def TranslateYabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n",
    "    assert 0 <= v\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n",
    "\n",
    "\n",
    "def Rotate(img, v):  # [-30, 30]\n",
    "    assert -30 <= v <= 30\n",
    "    if random.random() > 0.5:\n",
    "        v = -v\n",
    "    return img.rotate(v)\n",
    "\n",
    "\n",
    "def AutoContrast(img, _):\n",
    "    return PIL.ImageOps.autocontrast(img)\n",
    "\n",
    "\n",
    "def Invert(img, _):\n",
    "    return PIL.ImageOps.invert(img)\n",
    "\n",
    "\n",
    "def Equalize(img, _):\n",
    "    return PIL.ImageOps.equalize(img)\n",
    "\n",
    "\n",
    "def Flip(img, _):  # not from the paper\n",
    "    return PIL.ImageOps.mirror(img)\n",
    "\n",
    "\n",
    "def Solarize(img, v):  # [0, 256]\n",
    "    assert 0 <= v <= 256\n",
    "    return PIL.ImageOps.solarize(img, v)\n",
    "\n",
    "\n",
    "def SolarizeAdd(img, addition=0, threshold=128):\n",
    "    img_np = np.array(img).astype(np.int32) #!#np.int->np.int32\n",
    "    img_np = img_np + addition\n",
    "    img_np = np.clip(img_np, 0, 255)\n",
    "    img_np = img_np.astype(np.uint8)\n",
    "    img = Image.fromarray(img_np)\n",
    "    return PIL.ImageOps.solarize(img, threshold)\n",
    "\n",
    "\n",
    "def Posterize(img, v):  # [4, 8]\n",
    "    v = int(v)\n",
    "    v = max(1, v)\n",
    "    return PIL.ImageOps.posterize(img, v)\n",
    "\n",
    "\n",
    "def Contrast(img, v):  # [0.1,1.9]\n",
    "    assert 0.1 <= v <= 1.9\n",
    "    return PIL.ImageEnhance.Contrast(img).enhance(v)\n",
    "\n",
    "\n",
    "def Color(img, v):  # [0.1,1.9]\n",
    "    assert 0.1 <= v <= 1.9\n",
    "    return PIL.ImageEnhance.Color(img).enhance(v)\n",
    "\n",
    "\n",
    "def Brightness(img, v):  # [0.1,1.9]\n",
    "    assert 0.1 <= v <= 1.9\n",
    "    return PIL.ImageEnhance.Brightness(img).enhance(v)\n",
    "\n",
    "\n",
    "def Sharpness(img, v):  # [0.1,1.9]\n",
    "    assert 0.1 <= v <= 1.9\n",
    "    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n",
    "\n",
    "\n",
    "def Cutout(img, v):  # [0, 60] => percentage: [0, 0.2]\n",
    "    assert 0.0 <= v <= 0.2\n",
    "    if v <= 0.:\n",
    "        return img\n",
    "\n",
    "    v = v * img.size[0]\n",
    "    return CutoutAbs(img, v)\n",
    "\n",
    "\n",
    "def CutoutAbs(img, v):  # [0, 60] => percentage: [0, 0.2]\n",
    "    # assert 0 <= v <= 20\n",
    "    if v < 0:\n",
    "        return img\n",
    "    w, h = img.size\n",
    "    x0 = np.random.uniform(w)\n",
    "    y0 = np.random.uniform(h)\n",
    "\n",
    "    x0 = int(max(0, x0 - v / 2.))\n",
    "    y0 = int(max(0, y0 - v / 2.))\n",
    "    x1 = min(w, x0 + v)\n",
    "    y1 = min(h, y0 + v)\n",
    "\n",
    "    xy = (x0, y0, x1, y1)\n",
    "    color = (125, 123, 114)\n",
    "    # color = (0, 0, 0)\n",
    "    img = img.copy()\n",
    "    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n",
    "    return img\n",
    "\n",
    "\n",
    "def SamplePairing(imgs):  # [0, 0.4]\n",
    "    def f(img1, v):\n",
    "        i = np.random.choice(len(imgs))\n",
    "        img2 = PIL.Image.fromarray(imgs[i])\n",
    "        return PIL.Image.blend(img1, img2, v)\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def Identity(img, v):\n",
    "    return img\n",
    "\n",
    "\n",
    "def augment_list():  # 16 oeprations and their ranges\n",
    "    # https://github.com/google-research/uda/blob/master/image/randaugment/policies.py#L57\n",
    "    # l = [\n",
    "    #     (Identity, 0., 1.0),\n",
    "    #     (ShearX, 0., 0.3),  # 0\n",
    "    #     (ShearY, 0., 0.3),  # 1\n",
    "    #     (TranslateX, 0., 0.33),  # 2\n",
    "    #     (TranslateY, 0., 0.33),  # 3\n",
    "    #     (Rotate, 0, 30),  # 4\n",
    "    #     (AutoContrast, 0, 1),  # 5\n",
    "    #     (Invert, 0, 1),  # 6\n",
    "    #     (Equalize, 0, 1),  # 7\n",
    "    #     (Solarize, 0, 110),  # 8\n",
    "    #     (Posterize, 4, 8),  # 9\n",
    "    #     # (Contrast, 0.1, 1.9),  # 10\n",
    "    #     (Color, 0.1, 1.9),  # 11\n",
    "    #     (Brightness, 0.1, 1.9),  # 12\n",
    "    #     (Sharpness, 0.1, 1.9),  # 13\n",
    "    #     # (Cutout, 0, 0.2),  # 14\n",
    "    #     # (SamplePairing(imgs), 0, 0.4),  # 15\n",
    "    # ]\n",
    "\n",
    "    # https://github.com/tensorflow/tpu/blob/8462d083dd89489a79e3200bcc8d4063bf362186/models/official/efficientnet/autoaugment.py#L505\n",
    "    l = [\n",
    "        (AutoContrast, 0, 1),\n",
    "        (Equalize, 0, 1),\n",
    "        (Invert, 0, 1),\n",
    "        (Rotate, 0, 30),\n",
    "        (Posterize, 0, 4),\n",
    "        (Solarize, 0, 256),\n",
    "        (SolarizeAdd, 0, 110),\n",
    "        (Color, 0.1, 1.9),\n",
    "        (Contrast, 0.1, 1.9),\n",
    "        (Brightness, 0.1, 1.9),\n",
    "        (Sharpness, 0.1, 1.9),\n",
    "        (ShearX, 0., 0.3),\n",
    "        (ShearY, 0., 0.3),\n",
    "        (CutoutAbs, 0, 40),\n",
    "        (TranslateXabs, 0., 100),\n",
    "        (TranslateYabs, 0., 100),\n",
    "    ]\n",
    "\n",
    "    return l\n",
    "\n",
    "\n",
    "class Lighting(object):\n",
    "    \"\"\"Lighting noise(AlexNet - style PCA - based noise)\"\"\"\n",
    "\n",
    "    def __init__(self, alphastd, eigval, eigvec):\n",
    "        self.alphastd = alphastd\n",
    "        self.eigval = torch.Tensor(eigval)\n",
    "        self.eigvec = torch.Tensor(eigvec)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.alphastd == 0:\n",
    "            return img\n",
    "\n",
    "        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n",
    "        rgb = self.eigvec.type_as(img).clone() \\\n",
    "            .mul(alpha.view(1, 3).expand(3, 3)) \\\n",
    "            .mul(self.eigval.view(1, 3).expand(3, 3)) \\\n",
    "            .sum(1).squeeze()\n",
    "\n",
    "        return img.add(rgb.view(3, 1, 1).expand_as(img))\n",
    "\n",
    "\n",
    "class CutoutDefault(object):\n",
    "    \"\"\"\n",
    "    Reference : https://github.com/quark0/darts/blob/master/cnn/utils.py\n",
    "    \"\"\"\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h, w = img.size(1), img.size(2)\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "\n",
    "        y1 = np.clip(y - self.length // 2, 0, h)\n",
    "        y2 = np.clip(y + self.length // 2, 0, h)\n",
    "        x1 = np.clip(x - self.length // 2, 0, w)\n",
    "        x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "        mask[y1: y2, x1: x2] = 0.\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img *= mask\n",
    "        return img\n",
    "\n",
    "\n",
    "class RandAugment:\n",
    "    def __init__(self, n, m):\n",
    "        self.n = n\n",
    "        self.m = m      # [0, 30]\n",
    "        self.augment_list = augment_list()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        ops = random.choices(self.augment_list, k=self.n)\n",
    "        for op, minval, maxval in ops:\n",
    "            val = (float(self.m) / 30) * float(maxval - minval) + minval\n",
    "            img = op(img, val)\n",
    "\n",
    "        return img #!#\n",
    "    \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 64 #128 #!#\n",
    "learning_rate = 0.00099 #!0.00095 is the best\n",
    "root_dir = 'drive/app/cifar10/'\n",
    "default_directory = 'drive/app/torch/save_models/10_all_thing'\n",
    "\n",
    "writer = SummaryWriter('graph/10_all_thing') #!#\n",
    "\n",
    "# Data Augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_train.transforms.insert(0, RandAugment(2, 11)) #!#\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "# automatically download\n",
    "train_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                 train=True,\n",
    "                                 transform=transform_train,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=0)           # CPU loader number\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=0)            # CPU loader number\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class BottleNeck(nn.Module): #!#\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x  #!#\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet(BottleNeck, [3,4,6,3]) #!#\n",
    "\n",
    "modelses = model #!#\n",
    "modelses.cuda()#!#\n",
    "summary(modelses,(3,32,32)) #!#\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module): #!#\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim)) #!#\n",
    "\n",
    "optimizer = optim.Lamb( #!#\n",
    "    model.parameters(),\n",
    "    lr=learning_rate, #!#\n",
    "    #lr= 1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    #weight_decay=0, #!#\n",
    ") #!#\n",
    "criterion = LabelSmoothingLoss(classes = 10, smoothing = 0.1) #!#\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0): #!#\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if torch.cuda.is_available():\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam \n",
    "    \n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b) #!#\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "def rand_bbox(size, lam): #!cut\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int32(W * cut_rat) #!np.int -> np.int32\n",
    "    cut_h = np.int32(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2 #!cut\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            \n",
    "        r = np.random.rand(1) #!cut\n",
    "        if r < 0.5:\n",
    "            # generate mixed sample\n",
    "            lam = np.random.beta(1.0, 1.0)\n",
    "            rand_index = torch.randperm(data.size()[0]).cuda()\n",
    "            target_a = target\n",
    "            target_b = target[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "            data[:, :, bbx1:bbx2, bby1:bby2] = data[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            # adjust lambda to exactly match pixel ratio\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "            # compute output\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
    "        else:\n",
    "            # compute output\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target) #!cut\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        data, targets_a, targets_b, lam = mixup_data(data, target, alpha = 0.1) #!#\n",
    "        #output = model(data) #!cut\n",
    "        loss = mixup_criterion(criterion, output, targets_a, targets_b, lam) #!#\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "            print(\"lr: \", optimizer.param_groups[0]['lr']) #!#\n",
    "\n",
    "            writer.add_scalar('training loss', (train_loss / (batch_idx + 1)) , epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('training accuracy', (100. * correct / total), epoch * len(train_loader) + batch_idx) #!#\n",
    "            writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch * len(train_loader) + batch_idx) #!#\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "\n",
    "        writer.add_scalar('test loss', test_loss / (batch_idx + 1), epoch * len(test_loader)+ batch_idx) #!#\n",
    "        writer.add_scalar('test accuracy', 100. * correct / total, epoch * len(test_loader)+ batch_idx) #!#\n",
    "\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory)\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=320, eta_min=0, last_epoch=- 1) #!# T_max=320/total epoch=300/epoch=55 is the best\n",
    "\n",
    "for epoch in range(start_epoch, 300):\n",
    "\n",
    "    train(epoch)\n",
    "    if epoch < 55: #!#\n",
    "        lr = learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    else:\n",
    "        scheduler.step() #!#\n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    })\n",
    "    test(epoch)  #!#\n",
    "\n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os    \n",
    "import torchvision\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=0)           # CPU loader number\n",
    "\n",
    "# 이미지를 보여주기 위한 함수\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 학습용 이미지를 무작위로 가져오기\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"done_yet\")\n",
    "\n",
    "# 이미지 보여주기\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17087fd7bfc3788291c99d1c7467f8f2aa17d9b433eef99745cdb2b95be0a63d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('HW': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
